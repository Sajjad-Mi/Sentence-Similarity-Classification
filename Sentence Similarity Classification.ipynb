{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Mount Drive"],"metadata":{"id":"j0bdhMs4Rw0r"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"0jqxeRfLKuqj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Install Hazm"],"metadata":{"id":"_FpSP-i9SELw"}},{"cell_type":"code","source":["!mkdir resources\n","!wget -q \"https://github.com/sobhe/hazm/releases/download/v0.5/resources-0.5.zip\" -P resources\n","!unzip -qq resources/resources-0.5.zip -d resources\n","\n","!rm -rf /content/4ccae468eb73bf6c4f4de3075ddb5336\n","!rm -rf /content/preproc\n","!rm preprocessing.py utils.py\n","!mkdir -p /content/preproc\n","!git clone https://gist.github.com/4ccae468eb73bf6c4f4de3075ddb5336.git /content/preproc/\n","!mv /content/preproc/* /content/\n","!rm -rf /content/preproc\n","\n","!pip install hazm"],"metadata":{"id":"nF8FjdQ6SJMA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Import"],"metadata":{"id":"XwFCRf_JSLwR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"WhABxswiaK8c"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","from __future__ import unicode_literals\n","from hazm import *\n","from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n","from keras.utils import np_utils\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","from keras import Sequential, Model\n","from keras.layers import Embedding, GlobalAveragePooling1D, Dense, concatenate, Input, Flatten, LSTM, SimpleRNN, GRU, Dropout, MaxPooling1D, Conv1D\n","from keras.utils.vis_utils import plot_model\n","from sklearn.model_selection import train_test_split\n","import re\n","import tensorflow as tf\n","from sklearn.metrics import classification_report\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import accuracy_score\n","\n"]},{"cell_type":"markdown","source":["\n","# Ready Dataset"],"metadata":{"id":"bgFI8ElwEwsg"}},{"cell_type":"code","source":["path = '/content/drive/MyDrive/DL/Project 4/Dataset/PerSICK.csv'\n","df = pd.read_csv(path)"],"metadata":{"id":"BiUOv1_FK0_B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = df.dropna()\n","df = df.reset_index()"],"metadata":{"id":"wY-LKn3cI60l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head(10)"],"metadata":{"id":"EVL8ACQ1JVm0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","df['score'] = df['score'].round()\n","\n","sentence1 = df['sentence1']\n","sentence2 = df['sentence2']\n"],"metadata":{"id":"SV31KPCzlpwr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(sentence1))"],"metadata":{"id":"CUhIJ5UFps6P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Extract Subject"],"metadata":{"id":"QXwKkfa3Shpb"}},{"cell_type":"code","source":["word_tokenizer = WordTokenizer()\n","sentence_tokenizer = SentenceTokenizer()\n","\n","lexical_tokens_sent1 = [word_tokenizer.tokenize(sentence) for sentence in sentence1]\n","lexical_tokens_sent2 = [word_tokenizer.tokenize(sentence) for sentence in sentence2]\n"],"metadata":{"id":"sk0QggyHUwGE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(lexical_tokens_sent1[0])"],"metadata":{"id":"VbMDavVOOGtW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(lexical_tokens_sent2[0])"],"metadata":{"id":"zdas4H3wLQt6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["try:\n","    tagger = POSTagger(model='resources/postagger.model')\n","except:\n","    print('Instatiating POSTagger failed.')\n","pos_tagged_sent1 = [tagger.tag(lexical_token) for lexical_token in lexical_tokens_sent1]\n","pos_tagged_sent2 = [tagger.tag(lexical_token) for lexical_token in lexical_tokens_sent2]\n","\n"],"metadata":{"id":"2ZEhVu_JbtSV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pos_tagg_sent1 = []\n","pos_tagg_sent2 = []\n","#convert sentence to only tags\n","def convert_to_tag(tagged_sentencs):\n","  change_tag = {'AJ': 'A', 'PRO': 'R', 'Ne': 'N', 'AJe': 'A', 'NUM': 'U', 'CONJ': 'C', 'DET': 'D'}\n","  print(len(tagged_sentencs[0]))\n","  sentence_tag = []\n","  for i in range(len(tagged_sentencs)):\n","    sent_tag_temp = ''.join([each[1] if each[1] not in change_tag else change_tag[each[1]] for each in tagged_sentencs[i]])\n","\n","    sentence_tag.append(sent_tag_temp)\n","  return sentence_tag\n","\n","pos_tagg_sent1 = convert_to_tag(pos_tagged_sent1)\n","pos_tagg_sent2 = convert_to_tag(pos_tagged_sent2)\n","\n"],"metadata":{"id":"y41CFoyYRSBQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(pos_tagg_sent1[7]))\n","print(len(pos_tagged_sent1[7]))\n","print(pos_tagg_sent1[7])\n","print(pos_tagged_sent1[7])\n"],"metadata":{"id":"QwGJdj7Of8XC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["subj_pos_sent1 = []\n","subj_pos_sent2 = []\n","SBJ_pattern = '^(?!P)(P|U(P(A)?)?)?((N((A)|(N)+)*(A|N|R))|(N(A)?)|(R))(C)?'\n","#find subject from only tags sentence\n","\n","def find_pos(pattern, tag_sentence):\n","  subj_pos = []\n","  for i in range(len(tag_sentence)):\n","     pos_subj_temp = re.search(pattern, tag_sentence[i])\n","     subj_pos.append(pos_subj_temp)\n","  return subj_pos\n","\n","subj_pos_sent1 = find_pos(SBJ_pattern, pos_tagg_sent1)\n","subj_pos_sent2 = find_pos(SBJ_pattern, pos_tagg_sent2)"],"metadata":{"id":"sy_j9ZQioJUU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(subj_pos_sent1[9])"],"metadata":{"id":"xN5tCSlsVJ8U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["subj_index_sent1 = []\n","subj_index_sent2 = []\n","\n","def find_pos_index(subj_pos):\n","  pos_index = []\n","  for i in range(len(subj_pos)):\n","    if subj_pos[i] is not None:\n","      pos_index.append(subj_pos[i].span())\n","    else:\n","      pos_index.append(None)\n","  return pos_index\n","\n","subj_index_sent1 = find_pos_index(subj_pos_sent1)\n","subj_index_sent2 = find_pos_index(subj_pos_sent2)\n","\n","print(len(subj_index_sent1))"],"metadata":{"id":"avderU2moUzc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(pos_tagged_sent1[4])"],"metadata":{"id":"c33fBl-faUbm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["subj_sent1 = []\n","subj_sent2 = []\n","#find subject of sentece from their index\n","def find_words(words_index, tag_sentences):\n","  words = []\n","  words_temp = []\n","  for i in range(len(words_index)):\n","    if words_index[i] is not None:\n","      for j in range(words_index[i][0], words_index[i][1]):\n","        words_temp.append(tag_sentences[i][j][0])\n","      words.append(words_temp)\n","      words_temp = []\n","    else:\n","      words.append(['NA'])\n","  return words\n","\n","subj_sent1 = find_words(subj_index_sent1, pos_tagged_sent1)\n","subj_sent2 = find_words(subj_index_sent2, pos_tagged_sent2)\n","\n","print(len(subj_sent1))\n","print(len(subj_sent2))"],"metadata":{"id":"JxR0Z_fPoYnn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(subj_sent1[2])\n","print(subj_sent2[2])\n"],"metadata":{"id":"iP41H-sRj-_2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["str_subj_sent1 = []\n","str_subj_sent2 = []\n","#convert the subject word to string\n","def convert_to_string(sentence):\n","  str_sent = []\n","  for i in range(len(sentence)):\n","    str_temp = ''\n","    for j in range(len(sentence[i])):\n","      str_temp = str_temp + ' ' + sentence[i][j]\n","    str_sent.append(str_temp)\n","  return str_sent\n","\n","str_subj_sent1 = convert_to_string(subj_sent1)\n","str_subj_sent2 = convert_to_string(subj_sent2)\n"],"metadata":{"id":"ArJH2PZu8kVz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(str_subj_sent1))\n","print(len(str_subj_sent2))"],"metadata":{"id":"BRpecqyTCZtw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(str_subj_sent1[7])\n","print(str_subj_sent1[7])"],"metadata":{"id":"7TJNU6YMvFgU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","subj_class_sent1 = []\n","subj_class_sent2 = []\n","#classify every sentence based on their subject\n","def find_subject_class(sentence):\n","  subj_class = []\n","  for i in range(len(sentence)):\n","    if 'پسر' in sentence[i] or 'مرد' in sentence[i]:\n","      subj_class.append(6) \n","    elif 'زن' in sentence[i] or 'زن' in sentence[i]:\n","      subj_class.append(7)  \n","    elif 'کودک' in sentence[i]:\n","      subj_class.append(8)   \n","    elif 'سگ' in sentence[i] or 'گربه' in sentence[i]:\n","      subj_class.append(9)\n","    elif 'NA' in sentence[i]:\n","      subj_class.append(11)\n","    else:\n","       subj_class.append(10)\n","  return subj_class\n","\n","subj_class_sent1 = find_subject_class(str_subj_sent1)\n","subj_class_sent2 = find_subject_class(str_subj_sent2)\n"],"metadata":{"id":"2dvalIgLoYpt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(subj_class_sent1[:50])\n","print(subj_class_sent2[:50])"],"metadata":{"id":"Wr-7OmqWyOy7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["subj_class_sent = []\n","#classify every two sentence based on their subject \n","for i in range(len(subj_class_sent1)):\n","  if subj_class_sent1[i] != subj_class_sent2[i]:\n","    subj_class_sent.append(11)\n","  else:\n","    subj_class_sent.append(subj_class_sent1[i])\n","\n"],"metadata":{"id":"UvoNH9jHZo0O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(subj_class_sent)"],"metadata":{"id":"kocKDnVbyL9o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Preprocess Y"],"metadata":{"id":"BSCSQT_aumPJ"}},{"cell_type":"code","source":["#output of each row\n","y = [[int(y1), y2] for y1, y2 in zip(df['score'], subj_class_sent)]\n"],"metadata":{"id":"6ur8RwrJFeHi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(y))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R3lBep-MzhPu","executionInfo":{"status":"ok","timestamp":1654347679835,"user_tz":-270,"elapsed":417,"user":{"displayName":"سجاد محمودی","userId":"10760405173499806318"}},"outputId":"2b274975-67b4-4d45-fa91-de7399fc873d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3101\n"]}]},{"cell_type":"code","source":["print(y[7])\n","print(sentence1[7])\n","print(sentence2[7])"],"metadata":{"id":"wSpW75CjzmXl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#convert y to multi label binary\n","mlb = MultiLabelBinarizer()\n","mlb.fit(y)\n","y_OH = mlb.transform(y)\n","mlb.classes_\n","print(y_OH[7])"],"metadata":{"id":"0zDe6xXr0fjj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Implement with one hot encoding"],"metadata":{"id":"MorFR_lT6ZpX"}},{"cell_type":"code","source":["#build word vocabulary from most frequency word in dataset sentence\n","tk = Tokenizer(\n","    num_words=1500,\n","    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n","    split=' ',\n","    oov_token='UNK'\n",")\n","tk.fit_on_texts(sentence1)"],"metadata":{"id":"TNvuhk8t2Fpz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#map each word to its vocabulary index\n","X_cop1 = tk.texts_to_sequences(sentence1)\n","X_cop2 = tk.texts_to_sequences(sentence2)\n","X_cop1[0]\n","X_cop2[0]"],"metadata":{"id":"aMKyaRCt2myi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_len = max([len(sentence.split()) for sentence in sentence1])\n","max_len"],"metadata":{"id":"nknu2gvYa_j2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#fix sentences length\n","X_pad1 = pad_sequences(X_cop1, maxlen=20, padding='post')\n","X_pad2 = pad_sequences(X_cop2, maxlen=20, padding='post')\n","\n","X_pad1.shape"],"metadata":{"id":"kDSGj8UGbK-h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#convert sentences to one hot encode\n","X_OH1 = to_categorical(X_pad1, num_classes=1500)\n","X_OH2 = to_categorical(X_pad2, num_classes=1500)\n","\n","X_OH1.shape"],"metadata":{"id":"endzVLzzbURv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = [[s1, s2] for s1, s2 in zip(X_OH1, X_OH2)]\n"],"metadata":{"id":"eiqSPls38nkz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Split Data"],"metadata":{"id":"t6mI_iBILkqj"}},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(X, y_OH, test_size=0.2, shuffle=10)\n"],"metadata":{"id":"LAVSpmOQMiV8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train1 = [s1 for s1, s2 in X_train ]\n","X_train2= [s2 for s1, s2 in X_train ]\n","X_test1 = [s1 for s1, s2 in X_test ]\n","X_test2= [s2 for s1, s2 in X_test ]\n","\n","X_train1 = np.array(X_train1)\n","X_train2 = np.array(X_train2)\n","X_test1 = np.array(X_test1)\n","X_test2 = np.array(X_test2)\n","\n","X_test1.shape\n","X_train1.shape"],"metadata":{"id":"KYxVTjCtPSOp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## RNN Model"],"metadata":{"id":"HrcgsSwuQvmj"}},{"cell_type":"code","source":["model1_in = Input(shape=X_train1.shape[1:])\n","model1_out = SimpleRNN(50)(model1_in)  \n","\n","model1 = Model(model1_in, model1_out)\n","\n","model2_in = Input(shape=X_train2.shape[1:])\n","model2_out = SimpleRNN(50)(model2_in) \n","\n","model2 = Model(model2_in, model2_out)\n","\n","concatenated = concatenate([model1_out, model2_out])\n","layer = Flatten()(concatenated)\n","\n","layer = Dense(36, activation='relu')(layer)\n","layer = Dense(28, activation='relu')(layer)\n","out = Dense(11, activation='sigmoid', name='output_layer')(layer)\n","\n","model = Model([model1_in, model2_in], out)\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n","\n","\n","plot_model(model)\n"],"metadata":{"id":"U3b4tOuIIOBf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## GRU Model"],"metadata":{"id":"6gBs5E6RL_ap"}},{"cell_type":"code","source":["model1_in = Input(shape=X_train1.shape[1:])\n","\n","model1_out = GRU(50)(model1_in)              \n","model1 = Model(model1_in, model1_out)\n","\n","model2_in = Input(shape=X_train2.shape[1:])\n","model2_out = GRU(50)(model2_in) \n","\n","model2 = Model(model2_in, model2_out)\n","\n","\n","concatenated = concatenate([model1_out, model2_out])\n","\n","layer = Dense(30, activation='relu')(concatenated)\n","layer = Dense(26, activation='relu')(layer)\n","\n","out = Dense(11, activation='sigmoid', name='output_layer')(layer)\n","\n","model = Model([model1_in, model2_in], out)\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n","\n","\n","plot_model(model)\n"],"metadata":{"id":"3d4SSpPNMFbF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## LSTM Model"],"metadata":{"id":"q1lkSgy5FP_I"}},{"cell_type":"code","source":["model1_in = Input(shape=X_train1.shape[1:])\n","\n","model1_out = LSTM(50)(model1_in)              \n","model1 = Model(model1_in, model1_out)\n","\n","model2_in = Input(shape=X_train2.shape[1:])\n","model2_out = LSTM(50)(model2_in) \n","\n","model2 = Model(model2_in, model2_out)\n","\n","\n","concatenated = concatenate([model1_out, model2_out])\n","\n","layer = Dense(32, activation='relu')(concatenated)\n","layer = Dense(25, activation='relu')(layer)\n","\n","out = Dense(11, activation='sigmoid', name='output_layer')(layer)\n","\n","model = Model([model1_in, model2_in], out)\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n","\n","\n","plot_model(model)\n"],"metadata":{"id":"eADzgQykXpXo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train Model"],"metadata":{"id":"24vsHNHkMUPt"}},{"cell_type":"code","source":["history = model.fit([X_train1, X_train2], y=y_train, epochs=150,\n","             validation_split=0.2)"],"metadata":{"id":"55k2W3O1sC-5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluate train and test set "],"metadata":{"id":"as-gf6YhnFUK"}},{"cell_type":"code","source":["loss, accuracy = model.evaluate([X_train1, X_train2], y_train)\n","print('Accuracy of train set: %.2f' % (accuracy*100))\n","print('Loss of train set: %.3f' % (loss))"],"metadata":{"id":"XKMj-munmSsK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss, accuracy = model.evaluate([X_test1, X_test2], y_test)\n","print('Accuracy of test set: %.2f' % (accuracy*100))\n","print('Loss of test set: %.3f' % (loss))"],"metadata":{"id":"vJPUbkiURvdR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_test = [X_test1, X_test2]"],"metadata":{"id":"rAp61QkzYlZM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Implement with word embedding"],"metadata":{"id":"9e1n4ZtsR4eS"}},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.text import one_hot\n","from tensorflow.keras.layers import Embedding\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential"],"metadata":{"id":"jYDEE7-HF8Y3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentences= []\n","#join every two sentence\n","for i in range(len(sentence1)):\n","  sent = sentence1[i] + ' ' + sentence2[i]\n","  sentences.append(sent)"],"metadata":{"id":"-CQ_L-RqQqGp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(sentences[0])"],"metadata":{"id":"n3EuDO0MRiVB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_len = max([len(sentence.split()) for sentence in sentences])\n","max_len"],"metadata":{"id":"X4mxjUTESDrm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["words_len = 40\n","voc_size = 2000"],"metadata":{"id":"qIohNcYRtfbV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["onehot_repr=[one_hot(words, voc_size) for words in sentences] \n","pad_sent = pad_sequences(onehot_repr,padding='pre', maxlen=words_len)\n"],"metadata":{"id":"55ljmvLoTLOH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(onehot_repr[0])\n","print(pad_sent)"],"metadata":{"id":"OwIB1JZ7-44j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Split Data"],"metadata":{"id":"lzWHJI-r84Zf"}},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(pad_sent, y_OH, test_size=0.2, shuffle=10)\n"],"metadata":{"id":"WyACdC-RT0Be"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train.shape"],"metadata":{"id":"R5GajJ5cUG_l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## RNN Model"],"metadata":{"id":"tW24URAW-OBP"}},{"cell_type":"code","source":["\n","opt = tf.keras.optimizers.Adam(learning_rate=0.0001) \n","\n","model=Sequential()\n","model.add(Embedding(voc_size, 100, input_length=words_len))\n","model.add(SimpleRNN(50,  return_sequences=True,  input_shape=X_train.shape[1:]))\n","model.add(SimpleRNN(30,  return_sequences=True,  input_shape=X_train.shape[1:]))\n","model.add(SimpleRNN(20))\n","\n","model.add(Dense(35, activation='relu'))\n","model.add(Dense(28, activation='relu'))\n","model.add(Dense(28, activation='relu'))\n","model.add(Dense(28, activation='relu'))\n","\n","model.add(Dense(11, activation='sigmoid', name='output_layer'))\n","\n","\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n","\n","plot_model(model)"],"metadata":{"id":"SGZfv-WfG5nx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## GRU Model"],"metadata":{"id":"pMfg0RH9-Odg"}},{"cell_type":"code","source":["\n","opt = tf.keras.optimizers.Adam(learning_rate=0.0001) \n","\n","model=Sequential()\n","model.add(Embedding(voc_size, 100, input_length=words_len))\n","model.add(GRU(80,  return_sequences=True,  input_shape=X_train.shape[1:]))\n","model.add(GRU(50,  return_sequences=True,  input_shape=X_train.shape[1:]))\n","model.add(SimpleRNN(20))\n","\n","model.add(Dense(35, activation='relu'))\n","model.add(Dense(28, activation='relu'))\n","model.add(Dense(28, activation='relu'))\n","model.add(Dense(25, activation='relu'))\n","\n","model.add(Dense(11, activation='sigmoid', name='output_layer'))\n","\n","\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n","\n","plot_model(model)"],"metadata":{"id":"2YUAWiQkVOfk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## LSTM Model"],"metadata":{"id":"6ZsEZtfA-O3Z"}},{"cell_type":"code","source":["\n","opt = tf.keras.optimizers.Adam(learning_rate=0.0001) \n","\n","model=Sequential()\n","model.add(Embedding(voc_size, 100, input_length=words_len))\n","#model.add(LSTM(80,  return_sequences=True,  input_shape=X_train.shape[1:]))\n","model.add(LSTM(30))\n","\n","model.add(Dense(35, activation='relu'))\n","model.add(Dense(30, activation='relu'))\n","model.add(Dense(28, activation='relu'))\n","model.add(Dense(25, activation='relu'))\n","\n","model.add(Dense(11, activation='sigmoid', name='output_layer'))\n","\n","\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n","\n","plot_model(model)"],"metadata":{"id":"1S_RxEOPH9-p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train Model"],"metadata":{"id":"NhXH88Ic-huU"}},{"cell_type":"code","source":["history = model.fit(X_train, y=y_train, epochs=150,\n","             validation_split=0.2)"],"metadata":{"id":"sVODB5tzUHCV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluate train and test set "],"metadata":{"id":"jcTAIDX_Mg13"}},{"cell_type":"code","source":["yhat = model.predict(X_train)\n","yhat = yhat.round()\n","print(y_train[1800])\n","print(yhat[1800])\n"],"metadata":{"id":"QqYtRIRvaD-s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss, accuracy = model.evaluate(X_train, y_train)\n","print('Accuracy of train set: %.2f' % (accuracy*100))\n","print('Loss of train set: %.3f' % (loss))"],"metadata":{"id":"vRnFd2cUMOqJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss, accuracy = model.evaluate(X_test, y_test)\n","print('Accuracy of test set: %.2f' % (accuracy*100))\n","print('Loss of test set: %.3f' % (loss))"],"metadata":{"id":"b6rfLc7vMnUf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Plot"],"metadata":{"id":"esCgGHSuMuds"}},{"cell_type":"code","source":["history = history.history"],"metadata":{"id":"Mmm53n3PMw4D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(history['binary_accuracy'])\n","plt.xlabel('Epoch')\n","plt.ylabel('accuracy')\n","plt.show()\n","plt.plot(history['loss'], 'green')\n","plt.xlabel('Epoch')\n","plt.ylabel('loss')\n","plt.show()"],"metadata":{"id":"bSaph7HfMz-z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(history['val_binary_accuracy'])\n","plt.xlabel('Epoch')\n","plt.ylabel('accuracy')\n","plt.show()\n","plt.plot(history['val_loss'], 'green')\n","plt.xlabel('Epoch')\n","plt.ylabel('loss')\n","plt.show()"],"metadata":{"id":"l-dqEcPnM1o3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Test"],"metadata":{"id":"TAsWPAq3NFhR"}},{"cell_type":"code","source":["\n","labels =['1', '2', '3', '4' , '5', '6' , '7' , '8', '9', '10', '11']\n","y_predict = model.predict(X_test)\n","y_predict = y_predict.round()\n","\n","print(classification_report(y_test, y_predict, target_names=labels))"],"metadata":{"id":"gIx4af8v6jWq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["m = tf.keras.metrics.BinaryAccuracy()\n","m.update_state([[1], [1], [0], [0]], [[0.98], [1], [0.8], [0.8]])\n","m.result().numpy()\n"],"metadata":{"id":"fDR4yCIX-VQY"},"execution_count":null,"outputs":[]}]}